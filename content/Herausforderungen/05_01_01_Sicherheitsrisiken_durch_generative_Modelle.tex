
Generative KI-Modelle bringen spezifische neue Bedrohungen mit sich. Zu den
wichtigsten Risiken zählen sogenannte \enquote{Prompt Injections}, bei denen
durch manipulierte Eingaben unsicherer oder schädlicher Code erzeugt werden
kann, sowie \enquote{adversarial attacks}, bei denen minimale Änderungen an den
Eingabedaten zu sicherheitskritischen Verhaltensweisen führen können
\cite{shi_ai-assisted_2023}.

Ein weiteres Risiko besteht im sogenannten \enquote{Model Poisoning}, bei dem
während des Trainings gezielt fehlerhafte oder bösartige Daten eingespeist
werden, um Schwachstellen in der KI zu platzieren. Besonders große generative
Modelle wie LLMs sind hierfür anfällig, da sie oft auf umfangreiche und
öffentlich verfügbare Datenquellen zurückgreifen \cite{alwageed_role_nodate}.

Auch im Bereich der Software-Supply-Chain entstehen neue Risiken. Unzureichend
geprüfter oder von Dritten generierter Code kann unbemerkt Schwachstellen in
den Entwicklungsprozess einschleusen. Entlang der gesamten Kette, von der
Entwicklung bis zur Bereitstellung, muss daher auf Sicherheit und regelmäßige
Überprüfung geachtet werden \cite{siebert_generative_2024}.

Die Literatur empfiehlt, Sicherheitsmechanismen wie regelmäßige
Security-Reviews, Human-in-the-Loop-Prozesse und gezielte Trainingsmaßnahmen
gegen adversarielle Angriffe und Model Poisoning zu etablieren, um die
Robustheit generativer KI-Systeme zu erhöhen \cite{shi_ai-assisted_2023,
    alwageed_role_nodate, siebert_generative_2024}.
