
Beim Einsatz generativer KI in der Softwareentwicklung besteht eine zentrale
Herausforderung darin, dass große Sprachmodelle und generative Systeme
bestehende Vorurteile oder Stereotype aus ihren Trainingsdaten übernehmen und
unbewusst reproduzieren können. Dies führt zu unfairen, potenziell
diskriminierenden Ergebnissen und stellt ein erhebliches ethisches Risiko dar
\cite{weisz_design_2024}.

Um diesen Risiken zu begegnen, sind technische und organisatorische Maßnahmen
erforderlich. Dazu zählen regelmäßige Prüfungen auf Fairness und Transparenz,
der Einsatz von Diversity-Checks, kontrollierten Testdatensätzen sowie
spezialisierte Überwachungsmechanismen, um Verzerrungen zu erkennen und
abzumildern. Auch die Entwicklung und Durchsetzung von sogenannten Guardrails
in KI-Systemen wird als essenziell erachtet, um Diskriminierung und
Fehlinformationen zu verhindern \cite{weisz_design_2024,
    schmitt_generative_2024}.

Neben den technischen Aspekten hat Bias auch soziale und organisationale
Auswirkungen. Eine unkritische Nutzung von KI-Systemen kann im Team zu
Unsicherheiten, Vertrauensverlust und Spannungen führen – insbesondere dann,
wenn Vorschläge der KI als objektiv wahrgenommen werden, obwohl sie verzerrt
oder unvollständig sind \cite{schmitt_generative_2024}.

Insgesamt ist der verantwortungsvolle Umgang mit Bias und ethischen Konflikten
eine Grundvoraussetzung für den erfolgreichen und nachhaltigen Einsatz
generativer KI in der Softwareentwicklung.
