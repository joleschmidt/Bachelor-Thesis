Eines der zentralen ethischen Probleme beim Einsatz generativer KI in der
Softwareentwicklung ist die Gefahr von Bias und Diskriminierung. Wie Weisz et
al.~\cite{weisz_design_2024} herausstellen, können große Sprachmodelle und
generative Systeme bestehende Vorurteile, Diskriminierungen oder Stereotypen
aus den Trainingsdaten übernehmen und diese im erzeugten Code oder in den
Vorschlägen reproduzieren. Dies kann zu unfairen, potenziell diskriminierenden
Ergebnissen führen und damit ethische Grundsätze sowie
Gleichbehandlungsprinzipien verletzen.

Um solchen Risiken zu begegnen, empfehlen Weisz et
al.~\cite{weisz_design_2024}, dass Entwickler:innen und Organisationen
technische und organisatorische Maßnahmen (\enquote{Guardrails})
implementieren, die sicherstellen, dass generative KI-Lösungen regelmäßig auf
Fairness, Transparenz und mögliche Verzerrungen geprüft werden. Dazu zählen
etwa kontrollierte Testdatensätze, Diversity-Checks oder der Einsatz
spezialisierter Überwachungsmechanismen.

Schmitt et al.~\cite{schmitt_generative_2024} weisen darauf hin, dass die
Gefahr von Bias nicht nur technischer Natur ist, sondern auch soziale und
organisationale Auswirkungen haben kann. Insbesondere im Kontext beruflicher
Identität und Teamdynamik kann eine unkritische Nutzung von KI-Systemen zu
Unsicherheiten, Vertrauensverlust und Spannungen führen – etwa wenn Vorschläge
der KI als neutral oder objektiv wahrgenommen werden, obwohl sie verzerrt oder
unvollständig sind.

Insgesamt machen beide Quellen deutlich, dass ethische Konflikte und der Umgang
mit Bias zentrale Herausforderungen für den erfolgreichen und
verantwortungsvollen Einsatz generativer KI in der Softwareentwicklung
darstellen.
